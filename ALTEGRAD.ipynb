{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALTEGRAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YassineNJ/Mini-Game-Maze/blob/master/ALTEGRAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wNsOPqZLS45",
        "outputId": "e7de3db0-99e5-4fa3-c57f-7faebff9eb84"
      },
      "source": [
        "#!pip install -q pytorch-lightning\n",
        "!pip install -q -U dgl-cu101\n",
        "# !gdown --id \"1zAHe0nRAxzTsrBm3pNfqjQZeHd-uKgTd\"\n",
        "# !unzip -q \"/content/altegrad-2020.zip\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Altegrad_challenge/'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4LVQSBLFLj3",
        "outputId": "eddb605a-93d8-43a3-d287-431f5061cedb"
      },
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# # PyTorch Lightning\n",
        "# import pytorch_lightning as pl\n",
        "# from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "import dgl\n",
        "\n",
        "\n",
        "# Path to the folder where the pretrained models are saved\n",
        "# CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
        "\n",
        "# # Setting the seed\n",
        "# pl.seed_everything(42)\n",
        "\n",
        "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "# torch.backends.cudnn.determinstic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AqSTWYB6oWH",
        "outputId": "091a43aa-0c9f-4836-f628-ee220bf37b17"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import ast \r\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\r\n",
        "from nltk.corpus import stopwords \r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import re\r\n",
        "pattern = re.compile(r'(,){2,}')\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "\r\n",
        "fw = open(path + \"abstracts_processed.txt\",\"w\")\r\n",
        "f = open(path + \"abstracts.txt\",\"r\")\r\n",
        "\r\n",
        "# loads the inverted abstracts and stores them as id-abstracts in a dictionary dic and in a folder fw\r\n",
        "dim = 256\r\n",
        "dic = {}\r\n",
        "for l in f:\r\n",
        "    if(l==\"\\n\"):\r\n",
        "        continue\r\n",
        "    id = l.split(\"----\")[0]\r\n",
        "    inv = \"\".join(l.split(\"----\")[1:])\r\n",
        "    res = ast.literal_eval(inv) \r\n",
        "    abstract =[ \"\" for i in range(res[\"IndexLength\"])]\r\n",
        "    inv_indx=  res[\"InvertedIndex\"]\r\n",
        "\r\n",
        "    for i in inv_indx:\r\n",
        "        if i.isalpha() and i not in stop_words:\r\n",
        "            for j in inv_indx[i]:\r\n",
        "                abstract[j] = i.lower()\r\n",
        "    abstract = re.sub(pattern, ',', \",\".join(abstract))\r\n",
        "    fw.write(id+\"----\"+abstract+\"\\n\")\r\n",
        "    dic[id] = abstract\r\n",
        "fw.close()\r\n",
        "\r\n",
        "# cleans the abstracts from stopwords, numeric and non legible characters\r\n",
        "doc = []\r\n",
        "for i in dic:\r\n",
        "    p = dic[i].split(\",\")\r\n",
        "    dic[i] = [l for l in p if l.isalpha() and l not in stop_words]\r\n",
        "    doc.append(dic[i])\r\n",
        "\r\n",
        "# learns the embeddings of each abstract \r\n",
        "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(doc)]\r\n",
        "del doc\r\n",
        "model = Doc2Vec(tagged_data, vector_size = dim, window = 5, min_count = 2, epochs = 100, workers=10)\r\n",
        "\r\n",
        "# store the embeddings in \"paperID\":array format\r\n",
        "f = open(\"paper_embeddings.txt\",\"w\")\r\n",
        "for tid in dic:\r\n",
        "    sentence = dic[tid]\r\n",
        "    f.write(str(tid)+\":\"+np.array2string(model.infer_vector(sentence), formatter={'float_kind':lambda x: \"%.8f\" % x})+\"\\n\")    \r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PYaFuQRMr0U",
        "outputId": "a04e74e3-7f23-4382-89fd-b90bf7a5d3a0"
      },
      "source": [
        "path = '/content/drive/My Drive/Altegrad_challenge/'\n",
        "# read training data\n",
        "df_train_val = pd.read_csv(path + 'train.csv', dtype={'authorID': np.int64, 'h_index': np.float32})\n",
        "df_train, df_val = train_test_split(df_train_val, test_size=0.2)\n",
        "n_train = df_train.shape[0]\n",
        "n_val = df_val.shape[0]\n",
        "\n",
        "# read test data\n",
        "df_test = pd.read_csv(path + 'test.csv', dtype={'authorID': np.int64})\n",
        "n_test = df_test.shape[0]\n",
        "# full data \n",
        "df_full = pd.concat((df_train_val,df_test),axis=0)\n",
        "n_full = df_full.shape[0]\n",
        "# load the graph    \n",
        "G_nx = nx.read_edgelist(path + 'collaboration_network.edgelist', delimiter=' ', nodetype=int)\n",
        "n_nodes = G_nx.number_of_nodes()\n",
        "n_edges = G_nx.number_of_edges() \n",
        "print('Number of nodes:', n_nodes)\n",
        "print('Number of edges:', n_edges)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes: 231239\n",
            "Number of edges: 1777338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQxY-uVfjW-s"
      },
      "source": [
        "# computes structural features for each node\n",
        "core_number = nx.core_number(G_nx)\n",
        "avg_neighbor_degree = nx.average_neighbor_degree(G_nx)\n",
        "\n",
        "node_X = np.zeros((n_full, 3))\n",
        "\n",
        "for i,row in df_full.iterrows():\n",
        "    node = row['authorID']\n",
        "    node_X[i,0] = G_nx.degree(node)\n",
        "    node_X[i,1] = core_number[node]\n",
        "    node_X[i,2] = avg_neighbor_degree[node]\n",
        "\n",
        "node_X = torch.tensor(node_X,requires_grad=False,dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcvxfNkeMy6o"
      },
      "source": [
        "#G = dgl.from_networkx(G_nx)\n",
        "embed = nn.Embedding(G.number_of_nodes(), 100)  #embedding dim = 100\n",
        "G.ndata['feat'] = embed.weight\n",
        "\n",
        "#inputs = embed.weight.to(device)\n",
        "#inputs = node_X\n",
        "inputs = torch.cat([embed.weight,node_X],dim=1).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_a9UMQ9wuvK",
        "outputId": "aac40b6b-a0f3-4966-993a-1ea49b795545"
      },
      "source": [
        "embed.weight.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mNITcQeXUuT"
      },
      "source": [
        "G_cuda = G.to(device)\n",
        "######\n",
        "train_nodes = df_train[\"authorID\"].values\n",
        "val_nodes = df_val[\"authorID\"].values\n",
        "test_nodes = df_test[\"authorID\"].values\n",
        "#########\n",
        "train_labels = df_train[\"h_index\"].values\n",
        "val_labels = df_val[\"h_index\"].values\n",
        "train_labels=torch.from_numpy(train_labels).to(device)\n",
        "val_labels = torch.from_numpy(val_labels).to(device)\n",
        "\n",
        "###############\n",
        "nodes = list(G_nx.nodes())\n",
        "nodes_to_idx ={node:idx for idx,node in enumerate(nodes)}\n",
        "##############\n",
        "train_idxs = torch.LongTensor([nodes_to_idx.get(key) for key in train_nodes]).to(device)\n",
        "val_idxs = torch.LongTensor([nodes_to_idx.get(key) for key in val_nodes]).to(device)\n",
        "test_idxs = torch.LongTensor([nodes_to_idx.get(key) for key in test_nodes]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPmovn1oVcli",
        "outputId": "21084132-5b17-41eb-9afa-a0e7827fe8d6"
      },
      "source": [
        "from dgl.nn.pytorch import GraphConv\n",
        "from dgl.nn.pytorch.conv import GATConv\n",
        "\n",
        "class BasicGraphModel(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size, hidden_size, n_layers=2,output_size=1, nonlinearity=F.elu,n_heads=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        #self.layers.append(GraphConv(input_size, hidden_size, activation=nonlinearity))\n",
        "        self.layers.append(GATConv(input_size, hidden_size, num_heads=n_heads,\n",
        "                                   feat_drop=0.1, attn_drop=0.1, negative_slope=0.2, residual=True,\n",
        "                                   activation=nonlinearity, allow_zero_in_degree=False))\n",
        "        for i in range(n_layers - 1):\n",
        "            #self.layers.append(GraphConv(hidden_size, hidden_size, activation=nonlinearity))\n",
        "            self.layers.append(GATConv(hidden_size*n_heads, hidden_size, num_heads=n_heads,\n",
        "                                   feat_drop=0.1, attn_drop=0.1, negative_slope=0.2, residual=True,\n",
        "                                   activation=nonlinearity, allow_zero_in_degree=False))\n",
        "        \n",
        "\n",
        "        #self.layers.append(GraphConv(hidden_size, output_size))\n",
        "        self.layers.append(GATConv(hidden_size*n_heads,output_size, num_heads=1,\n",
        "                                   feat_drop=0.1, attn_drop=0.1, negative_slope=0.2, residual=True,\n",
        "                                   activation=nonlinearity, allow_zero_in_degree=False))\n",
        "        \n",
        "\n",
        "    def forward(self, g,inputs):\n",
        "        outputs = inputs\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            heads_out = layer(g, outputs)\n",
        "            #print(\"head\",i,\"outshape\",heads_out.size())\n",
        "            outputs = torch.flatten(heads_out, start_dim=1)\n",
        "            #print(\"after flatten head\",i,\"outshape\",outputs.size())\n",
        "        return outputs.squeeze()\n",
        "\n",
        "# The first layer transforms input features of size of 5 to a hidden size of 5.\n",
        "# The second layer transforms the hidden layer and produces output features of\n",
        "# size 2, corresponding to the two groups of the karate club.\n",
        "#net = GCN(inputs.size(1), 200)\n",
        "net = BasicGraphModel(inputs.size(1),200)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicGraphModel(\n",
              "  (layers): ModuleList(\n",
              "    (0): GATConv(\n",
              "      (fc): Linear(in_features=103, out_features=800, bias=False)\n",
              "      (feat_drop): Dropout(p=0.1, inplace=False)\n",
              "      (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "      (res_fc): Linear(in_features=103, out_features=800, bias=False)\n",
              "    )\n",
              "    (1): GATConv(\n",
              "      (fc): Linear(in_features=800, out_features=800, bias=False)\n",
              "      (feat_drop): Dropout(p=0.1, inplace=False)\n",
              "      (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "      (res_fc): Linear(in_features=800, out_features=800, bias=False)\n",
              "    )\n",
              "    (2): GATConv(\n",
              "      (fc): Linear(in_features=800, out_features=1, bias=False)\n",
              "      (feat_drop): Dropout(p=0.1, inplace=False)\n",
              "      (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "      (res_fc): Linear(in_features=800, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "V14mS5y3V6O2",
        "outputId": "b8a1cf22-93df-4648-9dd1-f46216895d65"
      },
      "source": [
        "import itertools\n",
        "\n",
        "optimizer = torch.optim.Adam(itertools.chain(net.parameters(), embed.parameters()))\n",
        "mse_loss = nn.MSELoss()\n",
        "all_preds = []\n",
        "for epoch in range(1000):\n",
        "    preds = net(G_cuda, inputs)\n",
        "    # we save the logits for visualization later\n",
        "    all_preds.append(preds.detach())\n",
        "    #logp = F.log_softmax(logits, 1)\n",
        "    # we only compute loss for labeled nodes\n",
        "    loss = mse_loss(preds[train_idxs], train_labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "          val_loss= mse_loss(preds[val_idxs], val_labels)\n",
        "          print('Epoch %d | Loss: %.4f | Test Loss: %.4f' % (epoch, loss.item(),val_loss.item()))\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 11317.4717 | Test Loss: 10711.3252\n",
            "Epoch 5 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 10 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 15 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 20 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 25 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 30 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 35 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 40 | Loss: 262.4024 | Test Loss: 276.8000\n",
            "Epoch 45 | Loss: 262.4024 | Test Loss: 276.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-21fe7f4510b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ycRWCUV0B2"
      },
      "source": [
        "# Pytorch Lightning + Optuna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JxUgQDDKu6p"
      },
      "source": [
        "# computes structural features for each node\n",
        "core_number = nx.core_number(G)\n",
        "avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
        "\n",
        "# create the training matrix. each node is represented as a vector of 3 features:\n",
        "# (1) its degree, (2) its core number and (3) the average degree of its neighbors\n",
        "X_train = np.zeros((n_train, 3))\n",
        "y_train = np.zeros(n_train)\n",
        "for i,row in df_train.iterrows():\n",
        "    node = row['authorID']\n",
        "    X_train[i,0] = G.degree(node)\n",
        "    X_train[i,1] = core_number[node]\n",
        "    X_train[i,2] = avg_neighbor_degree[node]\n",
        "    y_train[i] = row['h_index']\n",
        "\n",
        "# create the test matrix. each node is represented as a vector of 3 features:\n",
        "# (1) its degree, (2) its core number and (3) the average degree of its neighbors\n",
        "X_test = np.zeros((n_test, 3))\n",
        "for i,row in df_test.iterrows():\n",
        "    node = row['authorID']\n",
        "    X_test[i,0] = G.degree(node)\n",
        "    X_test[i,1] = core_number[node]\n",
        "    X_test[i,2] = avg_neighbor_degree[node]\n",
        "    \n",
        "\n",
        "# write the predictions to file\n",
        "# df_test['h_index_pred'].update(pd.Series(np.round_(y_pred, decimals=3)))\n",
        "# df_test.loc[:,[\"authorID\",\"h_index_pred\"]].to_csv('test_predictions.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOOFsNpnGNbS"
      },
      "source": [
        "class NodeLevelGNN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if model_name == \"MLP\":\n",
        "            self.model = MLPModel(**model_kwargs)\n",
        "        else:\n",
        "            self.model = GNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.model(x, edge_index)\n",
        "\n",
        "        # Only calculate the loss on the nodes corresponding to the mask\n",
        "        if mode == \"train\":\n",
        "            mask = data.train_mask\n",
        "        elif mode == \"val\":\n",
        "            mask = data.val_mask\n",
        "        elif mode == \"test\":\n",
        "            mask = data.test_mask\n",
        "        else:\n",
        "            assert False, \"Unknown forward mode: %s\" % mode\n",
        "\n",
        "        loss = self.loss_module(x[mask], data.y[mask])\n",
        "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
        "        return loss, acc\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # We use SGD here, but Adam works as well\n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}